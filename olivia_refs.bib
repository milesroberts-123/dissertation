@article{Luscombe2001,
   abstract = {A flood of data means that many of the challenges in biology are now challenges in computing. Bioinformatics, the application of computational techniques to analyse the information associated with biomolecules on a large-scale, has now firmly established itself as a discipline in molecular biology, and encompasses a wide range of subject areas from structural biology, genomics to gene expression studies. In this review we provide an introduction and overview of the current state of the field. We discuss the main principles that underpin bioinformatics analyses, look at the types of biological information and databases that are commonly used, and finally examine some of the studies that are being conducted, particularly with reference to transcription regulatory systems. Introduction},
   author = {N.M. Luscombe and D. Greenbaum and M. Gerstein},
   doi = {10.1055/S-0038-1638103/ID/JR1638103-62},
   issn = {0943-4747},
   issue = {01},
   journal = {Yearbook of Medical Informatics},
   month = {8},
   pages = {83-100},
   pmid = {27701604},
   publisher = {Georg Thieme Verlag KG},
   title = {What is bioinformatics? An introduction and overview},
   volume = {10},
   url = {http://www.thieme-connect.com/products/ejournals/html/10.1055/s-0038-1638103 http://www.thieme-connect.de/DOI/DOI?10.1055/s-0038-1638103},
   year = {2001},
}

@article{ellegren:hal-01900639,
  TITLE = {{Determinants of genetic diversity}},
  AUTHOR = {Ellegren, Hans and Galtier, Nicolas},
  URL = {https://hal.science/hal-01900639},
  JOURNAL = {{Nature Reviews Genetics}},
  PUBLISHER = {{Nature Publishing Group}},
  VOLUME = {17},
  NUMBER = {7},
  PAGES = {422 - 433},
  YEAR = {2016},
  MONTH = Jul,
  DOI = {10.1038/nrg.2016.58},
  PDF = {https://hal.science/hal-01900639/file/EG2016.pdf},
  HAL_ID = {hal-01900639},
  HAL_VERSION = {v1},
}

@book{Lewontin_1974, place={New York}, title={The genetic basis of Evolutionary Change}, publisher={Columbia University Press}, author={Lewontin, Richard C.}, year={1974}} 

@article{haubold-bernhard,
    author = {Haubold, Bernhard},
    title = "{Alignment-free phylogenetics and population genetics}",
    journal = {Briefings in Bioinformatics},
    volume = {15},
    number = {3},
    pages = {407-418},
    year = {2013},
    month = {11},
    abstract = "{Phylogenetics and population genetics are central disciplines in evolutionary biology. Both are based on comparative data, today usually DNA sequences. These have become so plentiful that alignment-free sequence comparison is of growing importance in the race between scientists and sequencing machines. In phylogenetics, efficient distance computation is the major contribution of alignment-free methods. A distance measure should reflect the number of substitutions per site, which underlies classical alignment-based phylogeny reconstruction. Alignment-free distance measures are either based on word counts or on match lengths, and I apply examples of both approaches to simulated and real data to assess their accuracy and efficiency. While phylogeny reconstruction is based on the number of substitutions, in population genetics, the distribution of mutations along a sequence is also considered. This distribution can be explored by match lengths, thus opening the prospect of alignment-free population genomics.}",
    issn = {1467-5463},
    doi = {10.1093/bib/bbt083},
    url = {https://doi.org/10.1093/bib/bbt083},
    eprint = {https://academic.oup.com/bib/article-pdf/15/3/407/452439/bbt083.pdf},
}

@ARTICLE{Buffalo2021-kk,
  title    = "Quantifying the relationship between genetic diversity and
              population size suggests natural selection cannot explain
              Lewontin's Paradox",
  author   = "Buffalo, Vince",
  abstract = "Neutral theory predicts that genetic diversity increases with
              population size, yet observed levels of diversity across
              metazoans vary only two orders of magnitude while population
              sizes vary over several. This unexpectedly narrow range of
              diversity is known as Lewontin's Paradox of Variation (1974).
              While some have suggested selection constrains diversity, tests
              of this hypothesis seem to fall short. Here, I revisit Lewontin's
              Paradox to assess whether current models of linked selection are
              capable of reducing diversity to this extent. To quantify the
              discrepancy between pairwise diversity and census population
              sizes across species, I combine previously-published estimates of
              pairwise diversity from 172 metazoan taxa with newly derived
              estimates of census sizes. Using phylogenetic comparative
              methods, I show this relationship is significant accounting for
              phylogeny, but with high phylogenetic signal and evidence that
              some lineages experience shifts in the evolutionary rate of
              diversity deep in the past. Additionally, I find a negative
              relationship between recombination map length and census size,
              suggesting abundant species have less recombination and
              experience greater reductions in diversity due to linked
              selection. However, I show that even assuming strong and abundant
              selection, models of linked selection are unlikely to explain the
              observed relationship between diversity and census sizes across
              species.",
  journal  = "Elife",
  volume   =  10,
  month    =  aug,
  year     =  2021,
  address  = "England",
  keywords = "Lewontin's Paradox; evolutionary biology; linked selection; none;
              phylogenetic comparative methods",
  language = "en"
}

@article{kmc3,
    author = {Kokot, Marek and Długosz, Maciej and Deorowicz, Sebastian},
    title = "{KMC 3: counting and manipulating k-mer statistics}",
    journal = {Bioinformatics},
    volume = {33},
    number = {17},
    pages = {2759-2761},
    year = {2017},
    month = {05},
    abstract = "{Counting all k-mers in a given dataset is a standard procedure in many bioinformatics applications. We introduce KMC3, a significant improvement of the former KMC2 algorithm together with KMC tools for manipulating k-mer databases. Usefulness of the tools is shown on a few real problems.Program is freely available at http://sun.aei.polsl.pl/REFRESH/kmc.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btx304},
    url = {https://doi.org/10.1093/bioinformatics/btx304},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/33/17/2759/49040995/bioinformatics\_33\_17\_2759.pdf},
}

@ARTICLE{slim3,
  title    = "Evolutionary Modeling in {SLiM} 3 for Beginners",
  author   = "Haller, Benjamin C and Messer, Philipp W",
  abstract = "The SLiM forward genetic simulation framework has proved to be a
              powerful and flexible tool for population genetic modeling.
              However, as a complex piece of software with many features that
              allow simulating a diverse assortment of evolutionary models, its
              initial learning curve can be difficult. Here we provide a
              step-by-step demonstration of how to build a simple evolutionary
              model in SLiM 3, to help new users get started. We will begin
              with a panmictic neutral model, and build up to a model of the
              evolution of a polygenic quantitative trait under selection for
              an environmental phenotypic optimum.",
  journal  = "Mol Biol Evol",
  volume   =  36,
  number   =  5,
  pages    = "1101--1109",
  month    =  may,
  year     =  2019,
  address  = "United States",
  keywords = "QTL evolution; modeling tutorial; population genetic simulation",
  language = "en"
}

@article{Marcais2011,
    author = {Marçais, Guillaume and Kingsford, Carl},
    title = "{A fast, lock-free approach for efficient parallel counting of occurrences of k-mers}",
    journal = {Bioinformatics},
    volume = {27},
    number = {6},
    pages = {764-770},
    year = {2011},
    month = {01},
    abstract = "{Motivation: Counting the number of occurrences of every k-mer (substring of length k) in a long string is a central subproblem in many applications, including genome assembly, error correction of sequencing reads, fast multiple sequence alignment and repeat detection. Recently, the deep sequence coverage generated by next-generation sequencing technologies has caused the amount of sequence to be processed during a genome project to grow rapidly, and has rendered current k-mer counting tools too slow and memory intensive. At the same time, large multicore computers have become commonplace in research facilities allowing for a new parallel computational paradigm.Results: We propose a new k-mer counting algorithm and associated implementation, called Jellyfish, which is fast and memory efficient. It is based on a multithreaded, lock-free hash table optimized for counting k-mers up to 31 bases in length. Due to their flexibility, suffix arrays have been the data structure of choice for solving many string problems. For the task of k-mer counting, important in many biological applications, Jellyfish offers a much faster and more memory-efficient solution.Availability: The Jellyfish software is written in C++ and is GPL licensed. It is available for download at http://www.cbcb.umd.edu/software/jellyfish.Contact:  gmarcais@umd.eduSupplementary information:  Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btr011},
    url = {https://doi.org/10.1093/bioinformatics/btr011},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/27/6/764/48866141/bioinformatics\_27\_6\_764.pdf},
}

@Article{BFCounter,
author={Melsted, P{\'a}ll
and Pritchard, Jonathan K.},
title={Efficient counting of k-mers in DNA sequences using a bloom filter},
journal={BMC Bioinformatics},
year={2011},
month={Aug},
day={10},
volume={12},
number={1},
pages={333},
abstract={Counting k-mers (substrings of length k in DNA sequence data) is an essential component of many methods in bioinformatics, including for genome and transcriptome assembly, for metagenomic sequencing, and for error correction of sequence reads. Although simple in principle, counting k-mers in large modern sequence data sets can easily overwhelm the memory capacity of standard computers. In current data sets, a large fraction-often more than 50{\%}-of the storage capacity may be spent on storing k-mers that contain sequencing errors and which are typically observed only a single time in the data. These singleton k-mers are uninformative for many algorithms without some kind of error correction.},
issn={1471-2105},
doi={10.1186/1471-2105-12-333},
url={https://doi.org/10.1186/1471-2105-12-333}
}

@Article{Illumina,
author={Schmeing, Stephan
and Robinson, Mark D.},
title={ReSeq simulates realistic Illumina high-throughput sequencing data},
journal={Genome Biology},
year={2021},
month={Feb},
day={19},
volume={22},
number={1},
pages={67},
abstract={In high-throughput sequencing data, performance comparisons between computational tools are essential for making informed decisions at each step of a project. Simulations are a critical part of method comparisons, but for standard Illumina sequencing of genomic DNA, they are often oversimplified, which leads to optimistic results for most tools. ReSeq improves the authenticity of synthetic data by extracting and reproducing key components from real data. Major advancements are the inclusion of systematic errors, a fragment-based coverage model and sampling-matrix estimates based on two-dimensional margins. These improvements lead to more faithful performance evaluations. ReSeq is available at https://github.com/schmeing/ReSeq.},
issn={1474-760X},
doi={10.1186/s13059-021-02265-7},
url={https://doi.org/10.1186/s13059-021-02265-7}
}

@ARTICLE{Donmez2008-lj,
  title    = "Polymorphism due to multiple amino acid substitutions at a codon
              site within Ciona savignyi",
  author   = "Donmez, Nilgun and Bazykin, Georgii A and Brudno, Michael and
              Kondrashov, Alexey S",
  abstract = "We compared two haploid genotypes of one Ciona savignyi
              individual and identified codons at which these genotypes differ
              by two nonsynonymous substitutions. Using the C. intestinalis
              genome as an outgroup, we showed that both substitutions tend to
              occur in the same genotype. Only in 53 (34.4\%) of 154 codons,
              one substitution occurred in each of the two genotypes, although
              77 (50\%) of such codons are to be expected if substitutions were
              independent. We considered two feasible evolutionary causes for
              the observed pattern: substitutions driven by positive selection
              and compensatory substitutions, as well as several potential
              biases. However, none of these explanations is fully compelling,
              and data on multiple genotypes of C. savignyi would help to
              elucidate the causes of this pattern.",
  journal  = "Genetics",
  volume   =  181,
  number   =  2,
  pages    = "685--690",
  month    =  dec,
  year     =  2008,
  address  = "United States",
  language = "en"
}

@misc{pacbPopulationGenetics,
	author = {},
	title = {{P}opulation genetics --- pacb.com},
	howpublished = {\url{https://www.pacb.com/population-genetics/}},
	year = {},
	note = {[Accessed 27-04-2024]},
}

@ARTICLE{Phan2015-iy,
  title    = "How genome complexity can explain the difficulty of aligning
              reads to genomes",
  author   = "Phan, Vinhthuy and Gao, Shanshan and Tran, Quang and Vo, Nam S",
  abstract = "BACKGROUND: Although it is frequently observed that aligning
              short reads to genomes becomes harder if they contain complex
              repeat patterns, there has not been much effort to quantify the
              relationship between complexity of genomes and difficulty of
              short-read alignment. Existing measures of sequence complexity
              seem unsuitable for the understanding and quantification of this
              relationship. RESULTS: We investigated several measures of
              complexity and found that length-sensitive measures of complexity
              had the highest correlation to accuracy of alignment. In
              particular, the rate of distinct substrings of length k, where k
              is similar to the read length, correlated very highly to
              alignment performance in terms of precision and recall. We showed
              how to compute this measure efficiently in linear time, making it
              useful in practice to estimate quickly the difficulty of
              alignment for new genomes without having to align reads to them
              first. We showed how the length-sensitive measures could provide
              additional information for choosing aligners that would align
              consistently accurately on new genomes. CONCLUSIONS: We formally
              established a connection between genome complexity and the
              accuracy of short-read aligners. The relationship between genome
              complexity and alignment accuracy provides additional useful
              information for selecting suitable aligners for new genomes.
              Further, this work suggests that the complexity of genomes
              sometimes should be thought of in terms of specific computational
              problems, such as the alignment of short reads to genomes.",
  journal  = "BMC Bioinformatics",
  volume   = "16 Suppl 17",
  number   = "Suppl 17",
  pages    = "S3",
  month    =  dec,
  year     =  2015,
  address  = "England",
  language = "en"
}

@BOOK{noauthor_2005-fe,
  title    = "Proceedings of the second workshop on real large distributed
              systems: December 13, 2005, San Francisco, {CA}, {USA}",
  year     =  2005,
  language = "en"
}

@misc{genomeHumanGenomic,
	author = {},
	title = {{H}uman {G}enomic {V}ariation --- genome.gov},
	howpublished = {\url{https://www.genome.gov/about-genomics/educational-resources/fact-sheets/human-genomic-variation}},
	year = {},
	note = {[Accessed 27-04-2024]},
}

@ARTICLE{Kent2002-xv,
  title     = "The Human Genome Browser at {UCSC}",
  author    = "Kent, W J",
  journal   = "Genome Res.",
  publisher = "Cold Spring Harbor Laboratory",
  volume    =  12,
  number    =  6,
  pages     = "996--1006",
  month     =  jun,
  year      =  2002
}

@ARTICLE{Schoch2020-li,
  title    = "{NCBI} Taxonomy: a comprehensive update on curation, resources
              and tools",
  author   = "Schoch, Conrad L and Ciufo, Stacy and Domrachev, Mikhail and
              Hotton, Carol L and Kannan, Sivakumar and Khovanskaya, Rogneda
              and Leipe, Detlef and Mcveigh, Richard and O'Neill, Kathleen and
              Robbertse, Barbara and Sharma, Shobha and Soussov, Vladimir and
              Sullivan, John P and Sun, Lu and Turner, Se{\'a}n and
              Karsch-Mizrachi, Ilene",
  abstract = "The National Center for Biotechnology Information (NCBI) Taxonomy
              includes organism names and classifications for every sequence in
              the nucleotide and protein sequence databases of the
              International Nucleotide Sequence Database Collaboration. Since
              the last review of this resource in 2012, it has undergone
              several improvements. Most notable is the shift from a single SQL
              database to a series of linked databases tied to a framework of
              data called NameBank. This means that relations among data
              elements can be adjusted in more detail, resulting in expanded
              annotation of synonyms, the ability to flag names with specific
              nomenclatural properties, enhanced tracking of publications tied
              to names and improved annotation of scientific authorities and
              types. Additionally, practices utilized by NCBI Taxonomy curators
              specific to major taxonomic groups are described, terms peculiar
              to NCBI Taxonomy are explained, external resources are
              acknowledged and updates to tools and other resources are
              documented. Database URL: https://www.ncbi.nlm.nih.gov/taxonomy.",
  journal  = "Database (Oxford)",
  volume   =  2020,
  month    =  jan,
  year     =  2020,
  address  = "England",
  language = "en"
}

@misc{murmurHash3,
	author = {Austin Appleby},
	title = {Murmurhash3},
	howpublished = {\url{https://github.com/aappleby/smhasher/wiki/MurmurHash3}},
	year = {2016}
}

@article{repetiveRegionHuman,
author = {Lord, Jenny and Turton, James and Medway, Christopher and Shi, Hui and Brown, Kristelle and Lowe, James and Mann, David and Pickering-Brown, Stuart and Kalsheker, Noor and Passmore, Peter and Morgan, Kevin},
year = {2012},
month = {12},
pages = {262-75},
title = {Next generation sequencing of CLU, PICALM and CR1: pitfalls and potential solutions},
volume = {3},
journal = {International journal of molecular epidemiology and genetics}
}

@article{slimTreeRecording,
author = {Haller, Benjamin C. and Galloway, Jared and Kelleher, Jerome and Messer, Philipp W. and Ralph, Peter L.},
title = {Tree-sequence recording in SLiM opens new horizons for forward-time simulation of whole genomes},
journal = {Molecular Ecology Resources},
volume = {19},
number = {2},
pages = {552-566},
keywords = {background selection, coalescent, genealogical history, pedigree recording, selective sweeps, tree sequences},
doi = {https://doi.org/10.1111/1755-0998.12968},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1755-0998.12968},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1755-0998.12968},
abstract = {Abstract There is an increasing demand for evolutionary models to incorporate relatively realistic dynamics, ranging from selection at many genomic sites to complex demography, population structure, and ecological interactions. Such models can generally be implemented as individual-based forward simulations, but the large computational overhead of these models often makes simulation of whole chromosome sequences in large populations infeasible. This situation presents an important obstacle to the field that requires conceptual advances to overcome. The recently developed tree-sequence recording method (Kelleher, Thornton, Ashander, \& Ralph, 2018), which stores the genealogical history of all genomes in the simulated population, could provide such an advance. This method has several benefits: (1) it allows neutral mutations to be omitted entirely from forward-time simulations and added later, thereby dramatically improving computational efficiency; (2) it allows neutral burn-in to be constructed extremely efficiently after the fact, using “recapitation”; (3) it allows direct examination and analysis of the genealogical trees along the genome; and (4) it provides a compact representation of a population's genealogy that can be analysed in Python using the msprime package. We have implemented the tree-sequence recording method in SLiM 3 (a free, open-source evolutionary simulation software package) and extended it to allow the recording of non-neutral mutations, greatly broadening the utility of this method. To demonstrate the versatility and performance of this approach, we showcase several practical applications that would have been beyond the reach of previously existing methods, opening up new horizons for the modelling and exploration of evolutionary processes.},
year = {2019}
}

@article{baumdicker2022efficient,
  title={Efficient ancestry and mutation simulation with msprime 1.0},
  author = {Baumdicker, Franz and Bisschop, Gertjan and Goldstein, Daniel
    and Gower, Graham and Ragsdale, Aaron P and Tsambos, Georgia and Zhu, Sha
    and Eldon, Bjarki and Ellerman, E Castedo and Galloway, Jared G
    and Gladstein, Ariella L and Gorjanc, Gregor and Guo, Bing
    and Jeffery, Ben and Kretzschumar, Warren W and Lohse, Konrad
    and Matschiner, Michael and Nelson, Dominic and Pope, Nathaniel S
    and Quinto-Cortés, Consuelo D and Rodrigues, Murillo F
    and Saunack, Kumar and Sellinger, Thibaut and Thornton, Kevin
    and van Kemenade, Hugo and Wohns, Anthony W and Wong, Yan
    and Gravel, Simon and Kern, Andrew D and Koskela, Jere
    and Ralph, Peter L and Kelleher, Jerome},
  journal={Genetics},
  volume={220},
  number={3},
  pages={iyab229},
  year={2022},
  publisher={Oxford University Press}
}

@article{fan2000summarycache,
  author={Li Fan and Pei Cao and Almeida, J. and Broder, A.Z.},
  journal={IEEE/ACM Transactions on Networking}, 
  title={Summary cache: a scalable wide-area Web cache sharing protocol}, 
  year={2000},
  volume={8},
  number={3},
  pages={281-293},
  keywords={Cache memory;Internet;Bandwidth;Computer science;Routing protocols;Telecommunication traffic;Particle measurements;Virtual prototyping;Bit rate;Data analysis},
  doi={10.1109/90.851975}}

@article{roy2014turtle,
    author = {Roy, Rajat Shuvro and Bhattacharya, Debashish and Schliep, Alexander},
    title = "{ Turtle: Identifying frequent k -mers with cache-efficient algorithms }",
    journal = {Bioinformatics},
    volume = {30},
    number = {14},
    pages = {1950-1957},
    year = {2014},
    month = {03},
    abstract = "{Motivation: Counting the frequencies of k -mers in read libraries is often a first step in the analysis of high-throughput sequencing data. Infrequent k -mers are assumed to be a result of sequencing errors. The frequent k -mers constitute a reduced but error-free representation of the experiment, which can inform read error correction or serve as the input to de novo assembly methods. Ideally, the memory requirement for counting should be linear in the number of frequent k -mers and not in the, typically much larger, total number of k -mers in the read library. Results: We present a novel method that balances time, space and accuracy requirements to efficiently extract frequent k -mers even for high-coverage libraries and large genomes such as human. Our method is designed to minimize cache misses in a cache-efficient manner by using a pattern-blocked Bloom filter to remove infrequent k -mers from consideration in combination with a novel sort-and-compact scheme, instead of a hash, for the actual counting. Although this increases theoretical complexity, the savings in cache misses reduce the empirical running times. A variant of method can resort to a counting Bloom filter for even larger savings in memory at the expense of false-negative rates in addition to the false-positive rates common to all Bloom filter-based approaches. A comparison with the state-of-the-art shows reduced memory requirements and running times. Availability and implementation: The tools are freely available for download at http://bioinformatics.rutgers.edu/Software/Turtle and http://figshare.com/articles/Turtle/791582 . Contact:  rajatroy@cs.rutgers.edu or schliep@cs.rutgers.eduSupplementary information:  Supplementary data are available at Bioinformatics online. }",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btu132},
    url = {https://doi.org/10.1093/bioinformatics/btu132},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/30/14/1950/48925248/bioinformatics\_30\_14\_1950.pdf},
}

@article{shi2010quality,
title = {Quality-score guided error correction for short-read sequencing data using CUDA},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {1129-1138},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.125},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910001262},
author = {Haixiang Shi and Bertil Schmidt and Weiguo Liu and Wolfgang Müller-Wittig},
keywords = {DNA sequencing, CUDA, High-through short-read assembly, Bioinformatics},
abstract = {Recently introduced new sequencing technologies can produce massive amounts of short-read data. Detection and correction of sequencing errors in this data is an important but time-consuming pre-processing step for de-novo genome assembly. In this paper, we demonstrate how the quality-score value associated with each base-call can be integrated in a CUDA-based parallel error correction algorithm. We show that quality-score guided error correction can improve the assembly accuracy of several datasets from the NCBI SRA (Short-Read Archive) in terms of N50-values as well as runtime. We further propose a number of improvements of to our previously published CUDA-EC algorithm to improve its runtime by a factor of up to 1.88.}
}